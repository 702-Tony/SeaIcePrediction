{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture # this will hide the cell output\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install plotly\n",
    "!pip install sklearn\n",
    "\n",
    "# Lets start with all of the imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import plotly\n",
    "import plotly.graph_objs as go \n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now lets input the methods\n",
    "# LINEAR REGRESSION #\n",
    "#####################\n",
    "def get_linear_pred(DF_DATASET):\n",
    "    X = DF_DATASET[['year','month','day','dayofyear']]\n",
    "    n_y = DF_DATASET[['n_extent']]\n",
    "    s_y = DF_DATASET[['s_extent']]\n",
    "\n",
    "    N_L_REGR = LinearRegression(normalize=True).fit(X, n_y)\n",
    "    S_L_REGR = LinearRegression(normalize=True).fit(X, s_y)\n",
    "    return N_L_REGR, S_L_REGR\n",
    "\n",
    "def get_prediction(month_, day_, year_, DF_DATASET, N_L_REGR, S_L_REGR):\n",
    "    date_to_pred = str(month_).zfill(2)+\"-\"+str(day_).zfill(2)+\"-\"+str(year_)# '03-01-2022'\n",
    "    day_to_predict = Dummy(date_to_pred)\n",
    "    # list of day of the year values + or - 15 days\n",
    "    add_subtract_list = get_add_subtract_days(month_, day_, year_, 15)\n",
    "    df_data = DF_DATASET.loc[(DF_DATASET['dayofyear'].isin(add_subtract_list))]\n",
    "    # this is will select days that are within the date range + or - 10 days\n",
    "    X = df_data[['year','month','day','dayofyear']]\n",
    "    n_y = df_data[['n_extent']]\n",
    "    s_y = df_data[['s_extent']]\n",
    "    # North Model Creation and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, n_y, test_size=0.3, shuffle=True)\n",
    "    n_l_regr = LinearRegression(normalize=True).fit(X_train, y_train)\n",
    "    n_score = n_l_regr.score(X_test, y_test)\n",
    "\n",
    "    # South Model Creation and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, s_y, test_size=0.3, shuffle=True)\n",
    "    s_l_regr = LinearRegression(normalize=True).fit(X_train, y_train)\n",
    "    s_score = s_l_regr.score(X_test, y_test)\n",
    "\n",
    "    # create an object for the prediction output\n",
    "    _d = {'year': [int(year_)],\n",
    "      'month': [int(month_)],\n",
    "      'day' : [int(day_)],\n",
    "     'dayofyear': day_of_year_getter(day_to_predict)[0]}\n",
    "    # pass that object for the prediction\n",
    "    north = N_L_REGR.predict(pd.DataFrame(data=_d))\n",
    "    south = S_L_REGR.predict(pd.DataFrame(data=_d))\n",
    "    return north, south, n_score, s_score\n",
    "# CLUSTERING #\n",
    "##############\n",
    "def scale_data(DF_DATASET):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(DF_DATASET)\n",
    "    scaled_data = scaler.transform(DF_DATASET)\n",
    "    return scaled_data\n",
    "def encode_labels(DF_DATASET, encode=True):\n",
    "    # if encode is false, then undo the encoding\n",
    "    try:\n",
    "        if encode == False:\n",
    "            # unencode\n",
    "            DF_DATASET['date'] = le.inverse_transform(DF_DATASET['date'])\n",
    "        else : # else encode the labels\n",
    "            # encode\n",
    "            le.fit(DF_DATASET['date'])\n",
    "            DF_DATASET['date'] = le.transform(DF_DATASET['date'])\n",
    "    except:\n",
    "        # this means that the average dataframe has been passed in\n",
    "        # inelegant, but it works...\n",
    "        if encode == False:\n",
    "            # unencode\n",
    "            DF_DATASET['hemisphere'] = le.inverse_transform(DF_DATASET['hemisphere'])\n",
    "        else:  # else encode the labels\n",
    "            # encode\n",
    "            le.fit(DF_DATASET['hemisphere'])\n",
    "            DF_DATASET['hemisphere'] = le.transform(DF_DATASET['hemisphere'])\n",
    "    return DF_DATASET\n",
    "\n",
    "def h_cluster_data(DF_DATASET, n_clusters=None):\n",
    "    # encode the labels\n",
    "    DATASET = encode_labels(DF_DATASET, True)\n",
    "    # Scale the Data\n",
    "    scaled_data = scale_data(DF_DATASET)\n",
    "    # instantiate the Clustering Model\n",
    "    model = AgglomerativeClustering(distance_threshold=0, n_clusters = n_clusters)\n",
    "    # Fit and transform the data\n",
    "    clusters = model.fit_predict(scaled_data)\n",
    "    DF_DATASET['cluster'] = clusters\n",
    "    DF_DATASET = encode_labels(DF_DATASET, False)\n",
    "    return DF_DATASET\n",
    "\n",
    "def k_cluster_data(DF_DATASET, n_clusters=None):\n",
    "    # encode the labels\n",
    "    DATASET = encode_labels(DF_DATASET, True)\n",
    "    # Scale the Data\n",
    "    scaled_data = scale_data(DF_DATASET)\n",
    "    # instantiate the Clustering Model\n",
    "    model = KMeans(n_clusters=n_clusters)\n",
    "    # fit and store the predictions\n",
    "    clusters = model.fit_predict(scaled_data)\n",
    "    # add a new column with the cluster values\n",
    "    DF_DATASET['cluster'] = clusters\n",
    "    # un encode the labels on the Dataframe\n",
    "    DF_DATASET = encode_labels(DF_DATASET, False)\n",
    "    return DF_DATASET, model\n",
    "\n",
    "def k_cluster_data2(DF_DATASET, n_clusters=None):\n",
    "    # lets prep the list\n",
    "    year_list_n = {}\n",
    "    year_list_s = {}\n",
    "    # this puts each year into its own list in the year_list dict\n",
    "    for year in range(1979, 2018):\n",
    "        item = DF_DATASET[DF_DATASET['year'] == year]\n",
    "        # This adds a dict inside of the year val\n",
    "        year_list_n[year] = {}\n",
    "        year_list_s[year] = {}\n",
    "        # then adds the values to the dict\n",
    "        # counts up to 366(for leap years...)\n",
    "        for dayofyear in range(1, 367):\n",
    "            n_it = item[item['dayofyear'] == dayofyear]\n",
    "            # empty vals for use in try statementlater\n",
    "            s_it_n = None\n",
    "            n_it_n = None\n",
    "            try:\n",
    "                # this will throw an exception if val is None\n",
    "                n_it_n = n_it['n_extent'].values.item()\n",
    "            except:\n",
    "                # so we want to store it as None anyways\n",
    "                n_it_n = None\n",
    "            try:\n",
    "                s_it_n = n_it['s_extent'].values.item()\n",
    "            except:\n",
    "                s_it_n = None\n",
    "            year_list_n[year][dayofyear] = n_it_n\n",
    "            year_list_s[year][dayofyear] = s_it_n\n",
    "    # create dataframe objs from dicts\n",
    "    df_yls = DataFrame(data=year_list_s)\n",
    "    df_yln = DataFrame(data=year_list_n)\n",
    "    # transpose vals\n",
    "    df_yls = df_yls.transpose()\n",
    "    df_yln = df_yln.transpose()\n",
    "    # reset the indexes\n",
    "    df_yln.reset_index(inplace=True)\n",
    "    df_yls.reset_index(inplace=True)\n",
    "    # create column name list for new dataframe\n",
    "    column_names = ['year']\n",
    "    for i in range(1, 367):\n",
    "        column_names.append(i)\n",
    "    # set column names on dataframes\n",
    "    df_yln.columns = column_names\n",
    "    df_yls.columns = column_names\n",
    "    # Now impute the columns\n",
    "    # This will fill in any NaN vals with the mean value from the column\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    # fills the missing values\n",
    "    df_yln = imputer.fit_transform(df_yln)\n",
    "    df_yls = imputer.fit_transform(df_yls)\n",
    "    # recreate the dataframes\n",
    "    df_yln = DataFrame(df_yln)\n",
    "    df_yls = DataFrame(df_yls)\n",
    "    # reset the column names\n",
    "    df_yln.columns = column_names\n",
    "    df_yls.columns = column_names\n",
    "    # Now for the Kmeans for the north...\n",
    "    kmean_n = KMeans(n_clusters=n_clusters)\n",
    "    kmean_n.fit(df_yln)\n",
    "    clusters = kmean_n.predict(df_yln)\n",
    "    df_yln['clusters'] = clusters\n",
    "    # ...and for the south\n",
    "    kmean_s = KMeans(n_clusters=n_clusters)\n",
    "    kmean_s.fit(df_yls)\n",
    "    clusters = kmean_s.predict(df_yls)\n",
    "    df_yls['clusters'] = clusters\n",
    "    # return both data frames with clusters as well as kmeans\n",
    "    return df_yln, df_yls ,kmean_n, kmean_s\n",
    "\n",
    "# DATA_PREP #\n",
    "#############\n",
    "def prep_data(source):\n",
    "\n",
    "    # PREPARE DATASET\n",
    "    df = pd.read_csv(source)\n",
    "    print('CSV Read')\n",
    "    # rename columns\n",
    "    df.columns = ['year', 'month','day', 'extent','missing','source_data','hemisphere']\n",
    "    # remove unnecessary columns, i.e. source data\n",
    "    df = df.drop(columns=['source_data'])\n",
    "    # split the data frames\n",
    "    df_n = df[df['hemisphere'] =='north'] # all north extents\n",
    "    df_s = df[df['hemisphere'] =='south'] # all south extents\n",
    "    # merge the dataframes together\n",
    "    df_n.columns = ['year','month','day','n_extent','n_missing', 'df_n']\n",
    "    df_s.columns = ['year','month','day','s_extent','s_missing', 'df_s']\n",
    "    DF_DATASET = df_n.merge(df_s)\n",
    "    DF_DATASET = DF_DATASET.drop(columns=['df_n', 'df_s'])\n",
    "    # Now add a column with the date string\n",
    "    DF_DATASET['date'] = df[['month','day','year']].astype(str).agg('-'.join, axis=1)\n",
    "    df['date'] = df[['month','day','year']].astype(str).agg('-'.join, axis=1)\n",
    "    # create dayof the year column\n",
    "\n",
    "    DF_DATASET['dayofyear'] = hlp.day_of_year_getter(DF_DATASET['date'])\n",
    "    return DF_DATASET, df\n",
    "\n",
    "def get_avgs(df):\n",
    "    # group rows by year and month and average the corresponding values\n",
    "    avg_df = df.groupby(['year','month','hemisphere']).agg([np.average])\n",
    "    # set index\n",
    "    avg_df.index = avg_df.index.set_names(['year', 'month', 'hemisphere'])\n",
    "    # reset index to fill in vals\n",
    "    avg_df.reset_index(inplace=True)\n",
    "    # rename cols\n",
    "    avg_df.columns = ['year', 'month', 'hemisphere', 'dayavg', 'extentavg', 'missingavg']\n",
    "    # remove nulls if any\n",
    "    avg_df.dropna()\n",
    "    return avg_df\n",
    "\n",
    "def prep_bydoy(DF_DATASET):\n",
    "    # lets prep the list\n",
    "    year_list = {}\n",
    "    year_list_s = {}\n",
    "    # this puts each year into its own list in the year_list dict\n",
    "    for year in range(1979, 2015, 5):\n",
    "        item = DF_DATASET[DF_DATASET['year'] == year]\n",
    "        year_list[year] = {}\n",
    "        year_list_s[year] = {}\n",
    "        for dayofyear in range(1, 366):\n",
    "            n_it = item[item['dayofyear'] == dayofyear]\n",
    "\n",
    "            s_it_n = None\n",
    "            n_it_n = None\n",
    "            try:\n",
    "                n_it_n = n_it['n_extent'].values.item()\n",
    "            except:\n",
    "                n_it_n = None\n",
    "            try:\n",
    "                s_it_n = n_it['s_extent'].values.item()\n",
    "            except:\n",
    "                s_it_n = None\n",
    "            year_list[year][dayofyear] = n_it_n\n",
    "            year_list_s[year][dayofyear] = s_it_n\n",
    "\n",
    "    # output is 1979, 1980, etc\n",
    "    df_yls = pd.DataFrame(data=year_list_s).transpose()\n",
    "    df_yln = pd.DataFrame(data=year_list).transpose()\n",
    "\n",
    "\n",
    "    return df_yls, df_yln\n",
    "\n",
    "# DATA_VIS #\n",
    "############\n",
    "\n",
    "def plotly_scatter_plot(_month, _day, _year, DF_DATASET, N_L_REGR, S_L_REGR):\n",
    "    day_of_year = hlp.single_day_oy_getter(_year,_month,_day)\n",
    "    # get days to plot\n",
    "    add_subtract_list = hlp.get_add_subtract_days(_month, _day, _year, 15)\n",
    "    # get days from dataset for plot\n",
    "    df_data = DF_DATASET.loc[(DF_DATASET['dayofyear'].isin(add_subtract_list))]\n",
    "    X = df_data[['dayofyear']]\n",
    "    y_n = df_data[['n_extent']]\n",
    "    s_n = df_data[['s_extent']]\n",
    "    # get predictions\n",
    "    n_prediction = N_L_REGR.predict([[_year, _month, _day ,day_of_year]])\n",
    "    s_prediction = S_L_REGR.predict([[_year, _month, _day ,day_of_year]])\n",
    "    # for hover templates\n",
    "    template_str = '<b>Extent</b>: %{y:.2f}' + '<br><b>Day of Year</b>: %{x}<br>' + '<b>Date</b>: %{text}'\n",
    "    # create scatter plot for northern historical data\n",
    "    trace = go.Scatter(\n",
    "        x=df_data['dayofyear'],\n",
    "        y=df_data['n_extent'],\n",
    "        opacity=0.6,\n",
    "        hovertemplate =template_str,\n",
    "        text=df_data['date'],\n",
    "        name=\"<i>Northern Historical</i>\",\n",
    "        mode='markers')\n",
    "    # create scatter plot for southern historical data\n",
    "    trace2 = go.Scatter(\n",
    "        x=df_data['dayofyear'],\n",
    "        y=df_data['s_extent'],\n",
    "        opacity=0.6,\n",
    "        hovertemplate=template_str,\n",
    "        text=df_data['date'],\n",
    "        name=\"<i>Southern Historical</i>\",\n",
    "        mode='markers')\n",
    "    # create scatter plots for predictions\n",
    "    import numpy as np\n",
    "    n_prediction = go.Scatter(\n",
    "        x=np.array([day_of_year]),\n",
    "        y=n_prediction[0],\n",
    "        mode='markers',\n",
    "        marker = dict(size=[20],color='#3136cc'),\n",
    "        visible=True,\n",
    "        name=\"<b>Northern Prediction</b>\")\n",
    "    s_prediction = go.Scatter(\n",
    "        x=np.array([day_of_year]),\n",
    "        y=s_prediction[0],\n",
    "        mode='markers',\n",
    "        marker = dict(size=[20], color='#db432c'),\n",
    "        visible=True,\n",
    "        name=\"<b>Southern Prediction</b>\")\n",
    "    # create list of plots\n",
    "    data = [trace, trace2, n_prediction, s_prediction]\n",
    "    # create json data for passing to html output\n",
    "    graph_json = json.dumps(data, cls=plotly.utils.PlotlyJSONEncoder)\n",
    "    return graph_json\n",
    "\n",
    "\n",
    "# creates a violin plot to be used for the data visualizations\n",
    "def plotly_violin_plot(start, end, rate, df):\n",
    "    yr_list = []\n",
    "    # generate a list with the start and end dates to be plotted.\n",
    "    for i in range(start, end, rate):\n",
    "        yr_list.append(i)\n",
    "\n",
    "    data_fr = df[df['year'].isin(yr_list)]\n",
    "\n",
    "    left = go.Violin(x=data_fr['year'][ data_fr['hemisphere'] == 'north' ],\n",
    "                        y=df['extent'][ df['hemisphere'] == 'north' ],\n",
    "                        legendgroup='north', scalegroup='north', name='north',\n",
    "                        side='negative',\n",
    "                        line_color='blue')\n",
    "    right = go.Violin(x=data_fr['year'][ data_fr['hemisphere'] == 'south' ],\n",
    "                        y=df['extent'][ df['hemisphere'] == 'south' ],\n",
    "                        legendgroup='south', scalegroup='south', name='south',\n",
    "                        side='positive',\n",
    "                        line_color='orange')\n",
    "\n",
    "    data = [left, right]\n",
    "    graph_json = json.dumps(data, cls=plotly.utils.PlotlyJSONEncoder)\n",
    "    return graph_json\n",
    "\n",
    "\n",
    "def plotly_heatmap(avg_df, hemisphere):\n",
    "    template_str = '<b>Extent</b>: %{z:.2f}' + '<br><b>Year</b>: %{x}<br>' + '<b>Month</b>: %{y}'\n",
    "    # takes a string as hemisphere and returns a heatmap JSON\n",
    "    avgs = avg_df[avg_df['hemisphere'] == hemisphere]\n",
    "    heat = go.Heatmap(x=avgs['year'],\n",
    "                      y=avgs['month'],\n",
    "                      z=avgs['extentavg'],\n",
    "                      hovertemplate=template_str,\n",
    "                      colorscale='Viridis',\n",
    "                      name='<b>'+hemisphere.capitalize()+'</b>')\n",
    "    data = [heat]\n",
    "    graph_json = json.dumps(data, cls=plotly.utils.PlotlyJSONEncoder)\n",
    "    return graph_json\n",
    "\n",
    "def plotly_kmeans_scatter(DF_DATASET):\n",
    "    # this will plot the Dataset with the kmeans cluster from data that was\n",
    "    # transformed so that each year was on an evenly matched 366 day year of values\n",
    "    tr_df = DF_DATASET.transpose()\n",
    "    tr_df.columns = tr_df.iloc[0]\n",
    "    tr_df.drop(['year'], inplace=True)\n",
    "    tr_df.reset_index(inplace=True)\n",
    "    tr_df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "    charts = []\n",
    "    for i in range(1979, 2018):\n",
    "        X = tr_df[i].index[:-1]\n",
    "        Y = tr_df[i][:-1]\n",
    "        cluster = int(tr_df[i][366].item())\n",
    "        colrs = ['#0d0887', '#46039f', '#7201a8', '#9c179e']\n",
    "        charts.append(\n",
    "            go.Scatter(x=X,\n",
    "                       y=Y,\n",
    "                       marker=dict(\n",
    "                           color=colrs[cluster],\n",
    "                           # colorscale='Viridis'\n",
    "                       ),\n",
    "                       legendgroup=cluster,\n",
    "                       opacity=0.6,\n",
    "                       mode='lines',\n",
    "                       text=i,\n",
    "                       name=i\n",
    "\n",
    "                       )\n",
    "        )\n",
    "    data = charts\n",
    "    graph_json = json.dumps(data, cls=plotly.utils.PlotlyJSONEncoder)\n",
    "    return graph_json\n",
    "\n",
    "# Create a dataframe with the DF_DATASET dataframe and averages made up from that\n",
    "def plotly_bar_plots(DF_DATASET, s):\n",
    "    df_year = DF_DATASET\n",
    "    df_year_agg = df_year.groupby(['year','month']).agg([np.average])\n",
    "    df_year_agg.index = df_year_agg.index.set_names(['year', 'month'])\n",
    "    # reset index to fill in vals\n",
    "    df_year_agg.reset_index(inplace=True)\n",
    "    # rename cols\n",
    "    df_year_agg.columns = ['year','month','dayavg','n_ext','n_miss','s_ext','s_miss','dayoyavg','cls']\n",
    "    df_year_agg.head()\n",
    "\n",
    "    years = [1980,1985,1990, 1995, 2000, 2005, 2010, 2015, 2018]\n",
    "    data = []\n",
    "    data2 = []\n",
    "    # bar plot of extent values for each month of each year\n",
    "    for year in years:\n",
    "        df_year = df_year_agg[df_year_agg['year']==year]\n",
    "        north = str(year) + ' North'\n",
    "        data.append(go.Bar(\n",
    "            y=df_year['n_ext'],\n",
    "            x=df_year['month'],\n",
    "            name=north\n",
    "        ))\n",
    "        south = str(year) + ' South'\n",
    "        data2.append(go.Bar(\n",
    "            y=df_year['s_ext'],\n",
    "            x=df_year['month'],\n",
    "            name=south\n",
    "        ))\n",
    "    data = go.Figure(data=data)\n",
    "    data2 = go.Figure(data=data2)\n",
    "    n_graph_json = json.dumps(data, cls=plotly.utils.PlotlyJSONEncoder)\n",
    "    s_graph_json = json.dumps(data2, cls=plotly.utils.PlotlyJSONEncoder)\n",
    "    return n_graph_json, s_graph_json\n",
    "\n",
    "# HELPERS #\n",
    "###########\n",
    "\n",
    "def day_of_year_getter(date_vals):\n",
    "    # takes in an object with a list stored in the values attribute\n",
    "    str_list = date_vals.values\n",
    "    rt_list = []\n",
    "    for date_str in str_list:\n",
    "        date_format = '%m-%d-%Y'\n",
    "        current_date = datetime.strptime(date_str, date_format)\n",
    "        # day_delta = current_date - START_DATE\n",
    "        day_delta = current_date.timetuple().tm_yday\n",
    "        rt_list.append(day_delta)\n",
    "    return rt_list\n",
    "\n",
    "def date_transformer(date_str):\n",
    "    # this returns a date time obj\n",
    "    new_date\n",
    "    try:\n",
    "        new_date = parse(date_str)\n",
    "    except:\n",
    "        format = '%Y-%m-%d'\n",
    "        new_date = datetime.strptime(date_str, format)\n",
    "    return new_date\n",
    "\n",
    "def single_day_oy_getter(y, m, d):\n",
    "    # takes in month day and year vals\n",
    "    date_ = datetime(y, m, d)\n",
    "    day_delta = date_.timetuple().tm_yday\n",
    "    return day_delta\n",
    "\n",
    "def day_of_year_add_subtract(date_str, _days):\n",
    "    # takes in date string\n",
    "    date_format = '%m-%d-%Y'\n",
    "    current_date = datetime.strptime(date_str, date_format)\n",
    "    rt_list = []\n",
    "    plus_day = current_date + timedelta(days=_days)\n",
    "    rt_list.append(plus_day.timetuple().tm_yday)\n",
    "    minus_day = current_date - timedelta(days=_days)\n",
    "    rt_list.append(minus_day.timetuple().tm_yday)\n",
    "    # returns a list with 0 = plus_day and 1 = minus_day\n",
    "    return rt_list\n",
    "\n",
    "def get_add_subtract_days(month_, day_, year_, _days):\n",
    "    # this will return a list of daysofyear for plotting and linear regression\n",
    "    date_format = '%m-%d-%Y'\n",
    "    current_date = datetime(year_,month_, day_)\n",
    "    rt_list = []\n",
    "    # adds current_date\n",
    "    rt_list.append(current_date.timetuple().tm_yday)\n",
    "    for d in range(1, _days+1):\n",
    "        rt_list.append((current_date + timedelta(days=d)).timetuple().tm_yday)\n",
    "        rt_list.append((current_date - timedelta(days=d)).timetuple().tm_yday)\n",
    "    return rt_list\n",
    "\n",
    "class Dummy:\n",
    "    def __init__(self, _val):\n",
    "        self.values = []\n",
    "        self.values.append(_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets get the datasets built\n",
    "DF_DATASET, df = prep_data('app/dataset/seaice.csv')\n",
    "n_clusters = 4\n",
    "DF_DATASET, kmeans_model = k_cluster_data(DF_DATASET, n_clusters)\n",
    "year_col_df_north, year_col_df_south, kmeans_n, kmeans_s = k_cluster_data2(DF_DATASET, n_clusters)\n",
    "N_L_REGR, S_L_REGR = get_linear_pred(DF_DATASET)\n",
    "# get Avgs\n",
    "avg_df = get_avgs(df)\n",
    "avg_df, avg_kmeans = k_cluster_data(avg_df, n_clusters)\n",
    "\n",
    "print('#######' * 3, 'DF_DATASET', '#######' * 3)\n",
    "print(DF_DATASET.head())\n",
    "print('#######' * 3, 'Averages By Month', '#######' * 3)\n",
    "print(avg_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now for the Data Visualizations\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
